import torch
import torch.nn as nn
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from medmnist import INFO, PathMNIST

# 1. Data
data_transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor()
])

train_dataset = PathMNIST(split='train', transform=data_transform)
unlabelled_dataset = PathMNIST(split='train', transform=data_transform) # for self-supervised without labels
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
unlabelled_loader = DataLoader(unlabelled_dataset, batch_size=64, shuffle=True)

# 2. Base Model
backbone = models.resnet18(pretrained=False)
backbone.fc = nn.Identity()
projection_head = nn.Sequential(
    nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, 64)
)

class SimCLRModel(nn.Module):
    def __init__(self, backbone, projection_head):
        super().__init__()
        self.backbone = backbone
        self.head = projection_head
    def forward(self, x):
        features = self.backbone(x)
        projection = self.head(features)
        return projection

model = SimCLRModel(backbone, projection_head).cuda()

# 3. Contrastive loss (NT-Xent)
def nt_xent_loss(out_1, out_2, temperature=0.5):
    batch_size = out_1.shape[0]
    out_1 = nn.functional.normalize(out_1, dim=1)
    out_2 = nn.functional.normalize(out_2, dim=1)
    out = torch.cat([out_1, out_2], dim=0)
    sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)
    mask = ~torch.eye(2*batch_size, 2*batch_size, dtype=bool).cuda()
    sim_matrix = sim_matrix.masked_select(mask).view(2*batch_size, -1)
    pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)
    pos_sim = torch.cat([pos_sim, pos_sim], dim=0)
    loss = (-torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()
    return loss

# 4. Augmentations (SimCLR style)
augmentation = transforms.Compose([
    transforms.RandomResizedCrop(32),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),
    transforms.ToTensor()
])

# 5. Self-supervised Pre-training Loop
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(10):
    model.train()
    for (batch, _) in unlabelled_loader:
        x1 = augmentation(batch)
        x2 = augmentation(batch)
        out1 = model(x1.cuda())
        out2 = model(x2.cuda())
        loss = nt_xent_loss(out1, out2)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}: SimCLR loss {loss.item():.4f}")

# 6. Fine-tune with a simple classifier (additional example available)
